{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8747e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import numpy as np\n",
    "from statsmodels.api import Logit, add_constant\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "fe46ae5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_PATH = 'output/uber_rides_enriched.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "4f20b489",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "        CSV_PATH,\n",
    "        parse_dates=['start_at','end_at','arrived_at','time_grid'],\n",
    "    )\n",
    "\n",
    "# Localizamos las fechas en la zona horaria de Lima\n",
    "for col in ['start_at', 'end_at', 'arrived_at', 'time_grid']:\n",
    "    df[col] = df[col].dt.tz_localize(\n",
    "        'America/Lima',\n",
    "        ambiguous='NaT',\n",
    "        nonexistent='shift_forward'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6d7276",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(\n",
    "    columns=[\n",
    "        'duration',\n",
    "        'distance'\n",
    "    ],\n",
    "    inplace=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "7c4f8adc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 23111 entries, 0 to 23110\n",
      "Data columns (total 38 columns):\n",
      " #   Column                  Non-Null Count  Dtype                       \n",
      "---  ------                  --------------  -----                       \n",
      " 0   user_id                 23111 non-null  object                      \n",
      " 1   driver_id               19726 non-null  object                      \n",
      " 2   icon                    23111 non-null  object                      \n",
      " 3   start_type              23111 non-null  object                      \n",
      " 4   start_at                23111 non-null  datetime64[ns, America/Lima]\n",
      " 5   start_lat               22212 non-null  float64                     \n",
      " 6   start_lon               22212 non-null  float64                     \n",
      " 7   end_at                  22835 non-null  datetime64[ns, America/Lima]\n",
      " 8   end_lat                 22212 non-null  float64                     \n",
      " 9   end_lon                 22212 non-null  float64                     \n",
      " 10  end_state               23099 non-null  object                      \n",
      " 11  driver_start_lat        18310 non-null  float64                     \n",
      " 12  driver_start_lon        18310 non-null  float64                     \n",
      " 13  arrived_at              17716 non-null  datetime64[ns, America/Lima]\n",
      " 14  price                   22713 non-null  float64                     \n",
      " 15  price_distance          19941 non-null  float64                     \n",
      " 16  price_duration          19941 non-null  float64                     \n",
      " 17  distance                22848 non-null  float64                     \n",
      " 18  duration                22848 non-null  float64                     \n",
      " 19  cost                    21760 non-null  float64                     \n",
      " 20  cost_distance           18038 non-null  float64                     \n",
      " 21  cost_duration           18038 non-null  float64                     \n",
      " 22  source                  22988 non-null  object                      \n",
      " 23  driver_score            7650 non-null   float64                     \n",
      " 24  rider_score             15390 non-null  float64                     \n",
      " 25  dist_driver_to_start_m  18310 non-null  float64                     \n",
      " 26  time_driver_to_start_s  18310 non-null  float64                     \n",
      " 27  dist_start_to_end_m     22212 non-null  float64                     \n",
      " 28  time_start_to_end_s     22212 non-null  float64                     \n",
      " 29  lat_grid                13026 non-null  float64                     \n",
      " 30  lon_grid                13026 non-null  float64                     \n",
      " 31  time_grid               13925 non-null  datetime64[ns, America/Lima]\n",
      " 32  u10                     13026 non-null  float64                     \n",
      " 33  v10                     13026 non-null  float64                     \n",
      " 34  d2m                     13026 non-null  float64                     \n",
      " 35  t2m                     13026 non-null  float64                     \n",
      " 36  tcc                     13026 non-null  float64                     \n",
      " 37  tp                      931 non-null    float64                     \n",
      "dtypes: datetime64[ns, America/Lima](4), float64(28), object(6)\n",
      "memory usage: 6.7+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "8cad9c11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se eliminan 533 (2.31%) filas de un total de 23111\n"
     ]
    }
   ],
   "source": [
    "# Filas con coordenadas nulas y end_state == 'drop off'\n",
    "mask = (\n",
    "    df['start_lat'].isna() &\n",
    "    (df['end_state'] == 'drop off')\n",
    ")\n",
    "\n",
    "# Cuántas filas son\n",
    "print(f\"Se eliminan {mask.sum()} ({mask.sum()/len(df):.2%}) filas de un total de {len(df)}\")\n",
    "\n",
    "# Eliminar esas filas\n",
    "df = df.loc[~mask].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "3c897d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_drop = df[\n",
    "    df.end_state == 'drop off'\n",
    "].copy()\n",
    "\n",
    "df_n = df[\n",
    "    ~(df.end_state == 'drop off')\n",
    "].drop(\n",
    "    columns=[\n",
    "        'end_lat',\n",
    "        'end_lon',\n",
    "        'arrived_at',\n",
    "        'price_distance',\n",
    "        'price_duration',\n",
    "        'distance',\n",
    "        'duration',\n",
    "        'cost',\n",
    "        'cost_distance',\n",
    "        'cost_duration',\n",
    "        'driver_score',\n",
    "        'rider_score',\n",
    "        'dist_start_to_end_m',\n",
    "        'time_start_to_end_s',\n",
    "    ]\n",
    ").copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "c4767d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 16884 entries, 0 to 23108\n",
      "Data columns (total 38 columns):\n",
      " #   Column                  Non-Null Count  Dtype                       \n",
      "---  ------                  --------------  -----                       \n",
      " 0   user_id                 16884 non-null  object                      \n",
      " 1   driver_id               16884 non-null  object                      \n",
      " 2   icon                    16884 non-null  object                      \n",
      " 3   start_type              16884 non-null  object                      \n",
      " 4   start_at                16884 non-null  datetime64[ns, America/Lima]\n",
      " 5   start_lat               16884 non-null  float64                     \n",
      " 6   start_lon               16884 non-null  float64                     \n",
      " 7   end_at                  16880 non-null  datetime64[ns, America/Lima]\n",
      " 8   end_lat                 16884 non-null  float64                     \n",
      " 9   end_lon                 16884 non-null  float64                     \n",
      " 10  end_state               16884 non-null  object                      \n",
      " 11  driver_start_lat        16180 non-null  float64                     \n",
      " 12  driver_start_lon        16180 non-null  float64                     \n",
      " 13  arrived_at              16883 non-null  datetime64[ns, America/Lima]\n",
      " 14  price                   16879 non-null  float64                     \n",
      " 15  price_distance          16883 non-null  float64                     \n",
      " 16  price_duration          16883 non-null  float64                     \n",
      " 17  distance                16884 non-null  float64                     \n",
      " 18  duration                16884 non-null  float64                     \n",
      " 19  cost                    15953 non-null  float64                     \n",
      " 20  cost_distance           15078 non-null  float64                     \n",
      " 21  cost_duration           15078 non-null  float64                     \n",
      " 22  source                  16787 non-null  object                      \n",
      " 23  driver_score            7478 non-null   float64                     \n",
      " 24  rider_score             14930 non-null  float64                     \n",
      " 25  dist_driver_to_start_m  16180 non-null  float64                     \n",
      " 26  time_driver_to_start_s  16180 non-null  float64                     \n",
      " 27  dist_start_to_end_m     16884 non-null  float64                     \n",
      " 28  time_start_to_end_s     16884 non-null  float64                     \n",
      " 29  lat_grid                9886 non-null   float64                     \n",
      " 30  lon_grid                9886 non-null   float64                     \n",
      " 31  time_grid               9886 non-null   datetime64[ns, America/Lima]\n",
      " 32  u10                     9886 non-null   float64                     \n",
      " 33  v10                     9886 non-null   float64                     \n",
      " 34  d2m                     9886 non-null   float64                     \n",
      " 35  t2m                     9886 non-null   float64                     \n",
      " 36  tcc                     9886 non-null   float64                     \n",
      " 37  tp                      665 non-null    float64                     \n",
      "dtypes: datetime64[ns, America/Lima](4), float64(28), object(6)\n",
      "memory usage: 5.0+ MB\n"
     ]
    }
   ],
   "source": [
    "df_drop.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "2d0ee868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valores de espera a reemplazar por API: 10519\n",
      "Valores de viaje a reemplazar por API: 9034\n",
      "Total de filas a usar dato de API: 13537\n"
     ]
    }
   ],
   "source": [
    "# Series de valores calculados\n",
    "t_api_wait = df_drop['time_driver_to_start_s']\n",
    "t_calc_wait = (df_drop['arrived_at'] - df_drop['start_at']).dt.total_seconds().clip(lower=0)\n",
    "\n",
    "t_api_trip = df_drop['time_start_to_end_s']\n",
    "t_calc_trip = (\n",
    "    df_drop['end_at']\n",
    "    - pd.DataFrame({'1': df_drop['start_at'], '2': df_drop['arrived_at']})\n",
    "      .max(axis=1)\n",
    ")\n",
    "t_calc_trip = t_calc_trip.dt.total_seconds()\n",
    "\n",
    "# Error porcentual\n",
    "err_wait_pct = (t_api_wait - t_calc_wait).abs() / t_api_wait * 100\n",
    "err_trip_pct = (t_api_trip - t_calc_trip).abs() / t_api_trip * 100\n",
    "\n",
    "# Umbral del 20%\n",
    "mask_wait = err_wait_pct > 50\n",
    "mask_trip = err_trip_pct > 50\n",
    "mask_any  = mask_wait | mask_trip\n",
    "\n",
    "# Imprimir conteos\n",
    "print(f\"Valores de espera a reemplazar por API: {mask_wait.sum()}\")\n",
    "print(f\"Valores de viaje a reemplazar por API: {mask_trip.sum()}\")\n",
    "print(f\"Total de filas a usar dato de API: {mask_any.sum()}\")\n",
    "\n",
    "# DataFrame de filas para reemplazo\n",
    "df_flag = df_drop.loc[mask_any].copy()\n",
    "df_flag['err_wait_pct'] = err_wait_pct[mask_any]\n",
    "df_flag['err_trip_pct'] = err_trip_pct[mask_any]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "755a3e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filas modificadas: 13537\n"
     ]
    }
   ],
   "source": [
    "# Reemplazar las columnas de fechas por NaT en las filas que superen el umbral\n",
    "#   Para las discrepancias de espera, anulamos 'arrived_at' (así falla el cálculo de espera).\n",
    "df_drop.loc[mask_wait, ['arrived_at']] = pd.NaT\n",
    "\n",
    "#   Para las discrepancias de trayecto, anulamos 'end_at' (así falla el cálculo de duración de viaje).\n",
    "df_drop.loc[mask_trip, ['end_at']]      = pd.NaT\n",
    "\n",
    "# Verificación\n",
    "print(f\"Filas modificadas: {mask_any.sum()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "46ed8762",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_drop['hour'] = df_drop['start_at'].dt.hour\n",
    "df_drop['weekday'] = df_drop['start_at'].dt.weekday      # 0=Lunes … 6=Domingo\n",
    "df_drop['weekday_name'] = df_drop['start_at'].dt.day_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "c999c831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1) Crear wait_calc y trip_calc ---\n",
    "df_drop['wait_calc'] = (\n",
    "    df_drop['arrived_at'] - df_drop['start_at']\n",
    ").dt.total_seconds().clip(lower=0)\n",
    "df_drop['trip_calc'] = (\n",
    "    df_drop['end_at']\n",
    "    - pd.DataFrame({'1': df_drop['start_at'], '2': df_drop['arrived_at']})\n",
    "      .max(axis=1)\n",
    ").dt.total_seconds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa97bd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_features(\n",
    "    df, candidates, target,\n",
    "    corr_thresh=0.1,\n",
    "    imp_cum_thresh=0.9,\n",
    "    vif_thresh=10,\n",
    "    miss_pval=0.05,\n",
    "    min_complete_cases=20\n",
    "):\n",
    "    print(f\"\\n--- Selecting features for target '{target}' ---\")\n",
    "    # 1) DataFrame de casos completos para este target\n",
    "    cols_fc = candidates + [target]\n",
    "    df_comp = df[cols_fc].dropna()\n",
    "    print(f\"Complete cases for '{target}': {df_comp.shape[0]} rows\")\n",
    "    if df_comp.shape[0] < min_complete_cases:\n",
    "        print(f\"  Not enough complete cases (<{min_complete_cases}); returning all candidates.\")\n",
    "        return candidates.copy()\n",
    "\n",
    "    # 2) Missingness (Logistic MAR test)\n",
    "    flag = df[target].isna().astype(int)\n",
    "    X_miss = df[candidates].fillna(df[candidates].median())\n",
    "    try:\n",
    "        logit = Logit(flag, add_constant(X_miss)).fit(disp=False)\n",
    "        pvals = logit.pvalues.drop('const', errors='ignore')\n",
    "        miss_preds = pvals[pvals < miss_pval].index.tolist()\n",
    "        print(f\"Predictors of missingness (p < {miss_pval}): {miss_preds}\")\n",
    "    except Exception as e:\n",
    "        miss_preds = []\n",
    "        print(f\"  Could not run MAR test: {e}\")\n",
    "\n",
    "    # 3) Univariate correlation\n",
    "    corrs = df_comp.corr()[target].abs().drop(target)\n",
    "    corr_preds = corrs[corrs >= corr_thresh].index.tolist()\n",
    "    print(f\"Predictors by correlation ≥ {corr_thresh}: {corr_preds}\")\n",
    "\n",
    "    # 4) Initial union\n",
    "    init_preds = list(dict.fromkeys(miss_preds + corr_preds))\n",
    "    if not init_preds:\n",
    "        init_preds = candidates.copy()\n",
    "        print(\"  No predictors from MAR/corr; using all candidates.\")\n",
    "    print(f\"Initial candidate predictors: {init_preds}\")\n",
    "\n",
    "    # 5) RandomForest importances\n",
    "    X_rf, y_rf = df_comp[init_preds], df_comp[target]\n",
    "    rf = RandomForestRegressor(n_estimators=100, random_state=0, n_jobs=-1)\n",
    "    rf.fit(X_rf, y_rf)\n",
    "    imp = pd.Series(rf.feature_importances_, index=init_preds).sort_values(ascending=False)\n",
    "    cum_imp = imp.cumsum()\n",
    "    # select features up to cumulative importance threshold\n",
    "    sel = cum_imp[cum_imp <= imp_cum_thresh].index.tolist()\n",
    "    remaining = [f for f in imp.index if f not in sel]\n",
    "    if remaining and (cum_imp.iloc[len(sel)] < imp_cum_thresh):\n",
    "        sel.append(remaining[0])\n",
    "    if not sel:\n",
    "        sel = init_preds.copy()\n",
    "        print(\"  No features passed importance threshold; falling back to init_preds.\")\n",
    "\n",
    "    print(\"RandomForest importances and cumulative sums:\")\n",
    "    for feat, importance in imp.items():\n",
    "        print(f\"  {feat}: {importance:.4f} (cumulative: {cum_imp[feat]:.4f})\")\n",
    "    print(f\"Selected by importance (cum ≤ {imp_cum_thresh}): {sel}\")\n",
    "    if remaining:\n",
    "        print(f\"Dropped by importance: {remaining}\")\n",
    "\n",
    "    # 6) VIF filtering, dropping the least-important among high-VIF features\n",
    "    features = sel.copy()\n",
    "    if len(features) > 1:\n",
    "        try:\n",
    "            def compute_vif_matrix(X, feats):\n",
    "                arr = X[feats].dropna().values\n",
    "                return pd.Series(\n",
    "                    [variance_inflation_factor(arr, i) for i in range(len(feats))],\n",
    "                    index=feats\n",
    "                )\n",
    "\n",
    "            mat = df_comp[features].dropna().values\n",
    "            if mat.shape[0] > mat.shape[1]:\n",
    "                print(f\"Starting VIF filtering (threshold={vif_thresh}) on: {features}\")\n",
    "                vif_series = compute_vif_matrix(df_comp, features)\n",
    "                while True:\n",
    "                    high_vif = vif_series[vif_series > vif_thresh]\n",
    "                    if high_vif.empty or len(features) == 1:\n",
    "                        print(\"  No further VIF > threshold; done.\")\n",
    "                        break\n",
    "\n",
    "                    # drop the high-VIF feature with the lowest RF importance\n",
    "                    to_drop = imp.loc[high_vif.index].idxmin()\n",
    "                    print(f\"  Dropping '{to_drop}' (VIF={vif_series[to_drop]:.1f}, lowest imp={imp[to_drop]:.4f})\")\n",
    "                    features.remove(to_drop)\n",
    "\n",
    "                    mat = df_comp[features].dropna().values\n",
    "                    if mat.shape[0] <= mat.shape[1]:\n",
    "                        print(\"  Insufficient rows for further VIF; stopping.\")\n",
    "                        break\n",
    "                    vif_series = compute_vif_matrix(df_comp, features)\n",
    "\n",
    "                print(f\"Features after VIF filtering: {features}\")\n",
    "            else:\n",
    "                print(\"  Skipping VIF: not enough complete rows vs. features.\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Skipping VIF due to error: {e}\")\n",
    "    else:\n",
    "        print(\"  Skipping VIF: need at least 2 features to compute.\")\n",
    "\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae054b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Selecting features for target 'wait_calc' ---\n",
      "Complete cases for 'wait_calc': 304 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gerof\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\statsmodels\\base\\model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictors of missingness (p < 0.05): ['time_driver_to_start_s', 'dist_driver_to_start_m', 'hour', 'weekday', 't2m', 'tp', 'v10']\n",
      "Predictors by correlation ≥ 0.1: ['time_driver_to_start_s', 'dist_driver_to_start_m', 'hour', 'weekday', 't2m', 'u10', 'v10']\n",
      "Initial candidate predictors: ['time_driver_to_start_s', 'dist_driver_to_start_m', 'hour', 'weekday', 't2m', 'tp', 'v10', 'u10']\n",
      "RandomForest importances and cumulative sums:\n",
      "  time_driver_to_start_s: 0.6803 (cumulative: 0.6803)\n",
      "  dist_driver_to_start_m: 0.2095 (cumulative: 0.8898)\n",
      "  u10: 0.0286 (cumulative: 0.9184)\n",
      "  t2m: 0.0251 (cumulative: 0.9435)\n",
      "  v10: 0.0250 (cumulative: 0.9685)\n",
      "  tp: 0.0176 (cumulative: 0.9862)\n",
      "  weekday: 0.0083 (cumulative: 0.9945)\n",
      "  hour: 0.0055 (cumulative: 1.0000)\n",
      "Selected by importance (cum ≤ 0.9): ['time_driver_to_start_s', 'dist_driver_to_start_m']\n",
      "Dropped by importance: ['u10', 't2m', 'v10', 'tp', 'weekday', 'hour']\n",
      "Starting VIF filtering (threshold=10) on: ['time_driver_to_start_s', 'dist_driver_to_start_m']\n",
      "  Dropping 'dist_driver_to_start_m' (VIF=13.3, lowest imp=0.2095)\n",
      "  Skipping VIF due to error: zero-size array to reduction operation maximum which has no identity\n",
      "\n",
      "--- Selecting features for target 'trip_calc' ---\n",
      "Complete cases for 'trip_calc': 356 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gerof\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\statsmodels\\base\\model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictors of missingness (p < 0.05): ['time_start_to_end_s', 'dist_start_to_end_m', 'hour', 'weekday', 'tp']\n",
      "Predictors by correlation ≥ 0.1: ['time_start_to_end_s', 'dist_start_to_end_m', 'hour', 't2m', 'tp', 'u10']\n",
      "Initial candidate predictors: ['time_start_to_end_s', 'dist_start_to_end_m', 'hour', 'weekday', 'tp', 't2m', 'u10']\n",
      "RandomForest importances and cumulative sums:\n",
      "  time_start_to_end_s: 0.3641 (cumulative: 0.3641)\n",
      "  dist_start_to_end_m: 0.2975 (cumulative: 0.6616)\n",
      "  u10: 0.1084 (cumulative: 0.7700)\n",
      "  t2m: 0.0761 (cumulative: 0.8462)\n",
      "  tp: 0.0664 (cumulative: 0.9125)\n",
      "  weekday: 0.0505 (cumulative: 0.9630)\n",
      "  hour: 0.0370 (cumulative: 1.0000)\n",
      "Selected by importance (cum ≤ 0.9): ['time_start_to_end_s', 'dist_start_to_end_m', 'u10', 't2m']\n",
      "Dropped by importance: ['tp', 'weekday', 'hour']\n",
      "Starting VIF filtering (threshold=10) on: ['time_start_to_end_s', 'dist_start_to_end_m', 'u10', 't2m']\n",
      "  Dropping 't2m' (VIF=10.3, lowest imp=0.0761)\n",
      "  Dropping 'dist_start_to_end_m' (VIF=15.2, lowest imp=0.2975)\n",
      "  No further VIF > threshold; done.\n",
      "Features after VIF filtering: ['time_start_to_end_s', 'u10']\n",
      "Features selected for wait_calc:\n",
      "['time_driver_to_start_s']\n",
      "\n",
      "Features selected for trip_calc:\n",
      "['time_start_to_end_s', 'u10']\n"
     ]
    }
   ],
   "source": [
    "wait_candidates = [\n",
    "    'time_driver_to_start_s',      # API’s estimate of driver travel\n",
    "    'dist_driver_to_start_m',      # distance driver→pickup\n",
    "    'hour', 'weekday',             # temporal context at request\n",
    "    't2m', 'tp', 'u10', 'v10'  # weather at request\n",
    "]\n",
    "\n",
    "trip_candidates = [\n",
    "    'time_start_to_end_s',         # API’s estimate of trip\n",
    "    'dist_start_to_end_m',         # distance pickup→dropoff\n",
    "    'hour', 'weekday',             # temporal context at pickup\n",
    "    't2m', 'tp', 'u10', 'v10'  # weather at pickup\n",
    "]\n",
    "\n",
    "\n",
    "# --- 4) Ejecutar selección para cada target ---\n",
    "wait_features = select_features(df_drop, wait_candidates, 'wait_calc')\n",
    "trip_features = select_features(df_drop, trip_candidates, 'trip_calc')\n",
    "\n",
    "print(\"Features selected for wait_calc:\")\n",
    "print(wait_features)\n",
    "print(\"\\nFeatures selected for trip_calc:\")\n",
    "print(trip_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "f18bbef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gerof\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\impute\\_iterative.py:895: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wait_calc was imputed in 10520 rows\n",
      "trip_calc was imputed in 9038 rows\n"
     ]
    }
   ],
   "source": [
    "wait_feats = wait_features + ['wait_calc']\n",
    "trip_features.remove('u10')\n",
    "trip_feats = trip_features + ['trip_calc']\n",
    "\n",
    "# 2) Configure imputers\n",
    "imp_wait = IterativeImputer(\n",
    "    estimator=RandomForestRegressor(n_estimators=100, random_state=0, n_jobs=-1),\n",
    "    max_iter=10,\n",
    "    random_state=0\n",
    ")\n",
    "imp_trip = IterativeImputer(\n",
    "    estimator=RandomForestRegressor(n_estimators=100, random_state=0, n_jobs=-1),\n",
    "    max_iter=10,\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "# 3) Fit & transform wait_calc\n",
    "wait_array = imp_wait.fit_transform(df_drop[wait_feats])\n",
    "df_drop['wait_calc'] = wait_array[:, wait_feats.index('wait_calc')]\n",
    "\n",
    "# 4) Fit & transform trip_calc\n",
    "trip_array = imp_trip.fit_transform(df_drop[trip_feats])\n",
    "df_drop['trip_calc'] = trip_array[:, trip_feats.index('trip_calc')]\n",
    "\n",
    "# 5) Reconstruct arrived_at where needed\n",
    "mask_arr = df_drop['arrived_at'].isna()\n",
    "df_drop.loc[mask_arr, 'arrived_at'] = (\n",
    "    df_drop.loc[mask_arr, 'start_at']\n",
    "    + pd.to_timedelta(df_drop.loc[mask_arr, 'wait_calc'], unit='s')\n",
    ")\n",
    "\n",
    "# 6) Reconstruct end_at where needed\n",
    "trip_start = pd.DataFrame({\n",
    "    '1': df_drop['start_at'],\n",
    "    '2': df_drop['arrived_at']\n",
    "}).max(axis=1)\n",
    "mask_end = df_drop['end_at'].isna()\n",
    "df_drop.loc[mask_end, 'end_at'] = (\n",
    "    trip_start.loc[mask_end]\n",
    "    + pd.to_timedelta(df_drop.loc[mask_end, 'trip_calc'], unit='s')\n",
    ")\n",
    "\n",
    "# 7) Report\n",
    "print(f\"wait_calc was imputed in {mask_arr.sum()} rows\")\n",
    "print(f\"trip_calc was imputed in {mask_end.sum()} rows\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "77c5e16e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valores de espera a reemplazar por API: 421\n",
      "Valores de viaje a reemplazar por API: 4284\n",
      "Total de filas a usar dato de API: 4602\n"
     ]
    }
   ],
   "source": [
    "# Series de valores calculados\n",
    "t_api_wait = df_drop['time_driver_to_start_s']\n",
    "t_calc_wait = (df_drop['arrived_at'] - df_drop['start_at']).dt.total_seconds().clip(lower=0)\n",
    "\n",
    "t_api_trip = df_drop['time_start_to_end_s']\n",
    "t_calc_trip = (\n",
    "    df_drop['end_at']\n",
    "    - pd.DataFrame({'1': df_drop['start_at'], '2': df_drop['arrived_at']})\n",
    "      .max(axis=1)\n",
    ")\n",
    "t_calc_trip = t_calc_trip.dt.total_seconds()\n",
    "\n",
    "# Error porcentual\n",
    "err_wait_pct = (t_api_wait - t_calc_wait).abs() / t_api_wait * 100\n",
    "err_trip_pct = (t_api_trip - t_calc_trip).abs() / t_api_trip * 100\n",
    "\n",
    "# Umbral del 20%\n",
    "mask_wait = err_wait_pct > 50\n",
    "mask_trip = err_trip_pct > 50\n",
    "mask_any  = mask_wait | mask_trip\n",
    "\n",
    "# Imprimir conteos\n",
    "print(f\"Valores de espera a reemplazar por API: {mask_wait.sum()}\")\n",
    "print(f\"Valores de viaje a reemplazar por API: {mask_trip.sum()}\")\n",
    "print(f\"Total de filas a usar dato de API: {mask_any.sum()}\")\n",
    "\n",
    "# DataFrame de filas para reemplazo\n",
    "df_flag = df_drop.loc[mask_any].copy()\n",
    "df_flag['err_wait_pct'] = err_wait_pct[mask_any]\n",
    "df_flag['err_trip_pct'] = err_trip_pct[mask_any]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
